FROM apache/airflow:2.9.1

USER root

# 필요한 패키지 설치
RUN apt-get update && apt-get install -y \
    curl \
    coreutils \
    util-linux \
    grep \
    gawk \
    procps \
    bash \
    openjdk-17-jdk \
    && apt-get clean && rm -rf /var/lib/apt/lists/*

# Apache Spark 설치
RUN curl -o /tmp/spark.tgz https://dlcdn.apache.org/spark/spark-3.5.4/spark-3.5.4-bin-hadoop3.tgz \
    && tar -xvzf /tmp/spark.tgz -C /opt/ \
    && mv /opt/spark-3.5.4-bin-hadoop3 /opt/spark \
    && rm /tmp/spark.tgz

# 하둡-aws 및 aws-sdk-bundle JAR 파일 복사
RUN mkdir -p /opt/spark/jars && \
    curl -o /opt/spark/jars/hadoop-aws-3.3.2.jar \
    https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.2/hadoop-aws-3.3.2.jar && \
    curl -o /opt/spark/jars/aws-java-sdk-bundle-1.11.901.jar \
    https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.901/aws-java-sdk-bundle-1.11.901.jar

USER airflow

# Python 패키지 설치
RUN pip install apache-airflow-providers-apache-spark boto3 pandas

# JAVA_HOME OS별 환경 변수 설정
ARG TARGETARCH
ENV JAVA_HOME=/usr/lib/jvm/java-17-openjdk-${TARGETARCH:-amd64}

# 환경 변수 설정
ENV SPARK_HOME=/opt/spark
ENV PATH=$JAVA_HOME/bin:$SPARK_HOME/bin:$PATH
