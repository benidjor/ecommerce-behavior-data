{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark를 활용한 ecommerce 유저 행동 데이터 parquet 파일 변환 및 저장\n",
    "- [kaggle 데이터셋 링크](https://www.kaggle.com/datasets/mkechinov/ecommerce-behavior-data-from-multi-category-store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import pickle\n",
    "from joblib import dump, load\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_timestamp, date_format, col\n",
    "from pyspark.sql.dataframe import DataFrame as SparkDataFrame\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2019-Dec.csv',\n",
       " '2019-Nov.csv',\n",
       " '2019-Oct.csv',\n",
       " '2020-Apr.csv',\n",
       " '2020-Feb.csv',\n",
       " '2020-Jan.csv',\n",
       " '2020-Mar.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_files = os.listdir(\"data/raw_data\")\n",
    "raw_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. 데이터프레임을 dictionary로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_files_to_dict(spark, folder_path):\n",
    "    \"\"\"\n",
    "    지정된 폴더 내의 모든 CSV 파일을 읽어와서,\n",
    "    파일명(key)과 DataFrame(value)의 dictionary를 생성\n",
    "    \"\"\"\n",
    "    # CSV 파일 목록 생성 (확장자가 .csv인 파일 필터링)\n",
    "    # csv_files = [f for f in os.listdir(folder_path) if f.endswith(\".csv\")]\n",
    "    csv_files = os.listdir(folder_path)\n",
    "    \n",
    "    # 결과를 저장할 dictionary 초기화\n",
    "    csv_dict = {}\n",
    "    \n",
    "    # 각 CSV 파일을 읽어와 dictionary에 저장\n",
    "    for file_name in tqdm(csv_files, desc=\"Processing CSV files\"):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        df = spark.read \\\n",
    "            .option(\"header\", True) \\\n",
    "            .option(\"inferSchema\", True) \\\n",
    "            .csv(file_path)\n",
    "        raw_df_name = file_name.replace(\"-\", \"_\").replace(\".csv\", \"\").lower()\n",
    "        csv_dict[raw_df_name] = df\n",
    "        # DataFrame 내용 확인 (첫 5행 출력)\n",
    "        # df.show(5)\n",
    "    \n",
    "    return csv_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files: 100%|██████████| 7/7 [04:51<00:00, 41.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CSV 파일 Dictionary keys: ['2019_dec', '2019_nov', '2019_oct', '2020_apr', '2020_feb', '2020_jan', '2020_mar']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# SparkSession 생성\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MultiCSVReader\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 함수 호출하여 dictionary 생성\n",
    "csv_dict = load_csv_files_to_dict(spark, \"data/raw_data\")\n",
    "\n",
    "# 생성된 dictionary의 key (파일명) 출력\n",
    "print(\"\\nCSV 파일 Dictionary keys:\", list(csv_dict.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. 불러온 데이터셋 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- category_id: long (nullable = true)\n",
      " |-- category_code: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- user_session: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_dict[\"2019_oct\"].printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+-------------------+--------------------+--------+-----+---------+--------------------+\n",
      "|         event_time|event_type|product_id|        category_id|       category_code|   brand|price|  user_id|        user_session|\n",
      "+-------------------+----------+----------+-------------------+--------------------+--------+-----+---------+--------------------+\n",
      "|2019-10-01 09:00:00|      view|  44600062|2103807459595387724|                NULL|shiseido|35.79|541312140|72d76fde-8bb3-4e0...|\n",
      "|2019-10-01 09:00:00|      view|   3900821|2053013552326770905|appliances.enviro...|    aqua| 33.2|554748717|9333dfbd-b87a-470...|\n",
      "|2019-10-01 09:00:01|      view|  17200506|2053013559792632471|furniture.living_...|    NULL|543.1|519107250|566511c2-e2e3-422...|\n",
      "+-------------------+----------+----------+-------------------+--------------------+--------+-----+---------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_dict[\"2019_oct\"].show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+-----------+-------------+-------+-----+-------+------------+\n",
      "|event_time|event_type|product_id|category_id|category_code|  brand|price|user_id|user_session|\n",
      "+----------+----------+----------+-----------+-------------+-------+-----+-------+------------+\n",
      "|         0|         0|         0|          0|     13515609|6113008|    0|      0|           2|\n",
      "+----------+----------+----------+-----------+-------------+-------+-----+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_counts_oct = csv_dict[\"2019_oct\"].select([F.count(F.when(F.col(c).isNull(), c)).alias(c)\\\n",
    "                                            for c in csv_dict[\"2019_oct\"].columns])\n",
    "null_counts_oct.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape: (42448764, 9)\n"
     ]
    }
   ],
   "source": [
    "# 행의 개수\n",
    "num_rows_oct = csv_dict[\"2019_oct\"].count()\n",
    "\n",
    "# 열의 개수: csv_dict[\"2019_oct\"].columns 리스트의 길이 계산\n",
    "num_columns_oct = len(csv_dict[\"2019_oct\"].columns)\n",
    "\n",
    "print(\"DataFrame shape:\", (num_rows_oct, num_columns_oct))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+-----------+-------------+-------+-----+-------+------------+\n",
      "|event_time|event_type|product_id|category_id|category_code|  brand|price|user_id|user_session|\n",
      "+----------+----------+----------+-----------+-------------+-------+-----+-------+------------+\n",
      "|         0|         0|         0|          0|      6755873|8985057|    0|      0|         109|\n",
      "+----------+----------+----------+-----------+-------------+-------+-----+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_counts_apr = csv_dict[\"2020_apr\"].select([F.count(F.when(F.col(c).isNull(), c)).alias(c)\\\n",
    "                                            for c in csv_dict[\"2020_apr\"].columns])\n",
    "null_counts_apr.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape: (66589268, 9)\n"
     ]
    }
   ],
   "source": [
    "# 행의 개수\n",
    "num_rows_apr = csv_dict[\"2020_apr\"].count()\n",
    "\n",
    "# 열의 개수: csv_dict[\"2020_apr\"].columns 리스트의 길이 계산\n",
    "num_columns_apr = len(csv_dict[\"2020_apr\"].columns)\n",
    "\n",
    "print(\"DataFrame shape:\", (num_rows_apr, num_columns_apr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 파생 컬럼 생성\n",
    "- `event_time` 컬럼의 형태: `2019-12-01 09:00:00`\n",
    "- 이 컬럼을 날짜와 시간 컬럼으로 각각 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_event_time(csv_dict: dict, event_time_col: str = \"event_time\") -> dict:\n",
    "    \"\"\"\n",
    "    Spark DataFrame의 event_time 컬럼을 timestamp로 변환 후, \n",
    "    날짜(yyyy-MM-dd)와 시간(HH:mm:ss) 컬럼으로 분리하여 csv_dict의 각 DataFrame을 업데이트\n",
    "    \"\"\"\n",
    "    for key in tqdm(csv_dict.keys(), desc=\"Seperating Columns\"):\n",
    "        df: SparkDataFrame = csv_dict[key]\n",
    "        \n",
    "        # 1. event_time 컬럼을 to_timestamp()를 사용하여 timestamp 타입으로 변환\n",
    "        df = df.withColumn(event_time_col, to_timestamp(col(event_time_col)))\n",
    "        \n",
    "        # 2. 날짜 부분 추출: date_format()을 사용하여 \"yyyy-MM-dd\" 형식으로 날짜 추출\n",
    "        df = df.withColumn(\"event_time_ymd\", date_format(col(event_time_col), \"yyyy-MM-dd\"))\n",
    "        \n",
    "        # 3. 시간 부분 추출: date_format()을 사용하여 \"HH:mm:ss\" 형식으로 시간 추출\n",
    "        df = df.withColumn(\"event_time_hms\", date_format(col(event_time_col), \"HH:mm:ss\"))\n",
    "        \n",
    "        # 수정된 DataFrame을 dictionary에 업데이트\n",
    "        csv_dict[key] = df\n",
    "        \n",
    "    return csv_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seperating Columns: 100%|██████████| 7/7 [00:00<00:00, 55.95it/s]\n"
     ]
    }
   ],
   "source": [
    "csv_dict_seperated = separate_event_time(csv_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['2019_dec', '2019_nov', '2019_oct', '2020_apr', '2020_feb', '2020_jan', '2020_mar'])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_dict_seperated.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+-------------------+--------------------+-----+-------+---------+--------------------+--------------+--------------+\n",
      "|         event_time|event_type|product_id|        category_id|       category_code|brand|  price|  user_id|        user_session|event_time_ymd|event_time_hms|\n",
      "+-------------------+----------+----------+-------------------+--------------------+-----+-------+---------+--------------------+--------------+--------------+\n",
      "|2019-12-01 09:00:00|      view|   1005105|2232732093077520756|construction.tool...|apple|1302.48|556695836|ca5eefc5-11f9-450...|    2019-12-01|      09:00:00|\n",
      "|2019-12-01 09:00:00|      view|  22700068|2232732091643068746|                NULL|force| 102.96|577702456|de33debe-c7bf-44e...|    2019-12-01|      09:00:00|\n",
      "|2019-12-01 09:00:01|      view|   2402273|2232732100769874463|appliances.person...|bosch| 313.52|539453785|5ee185a7-0689-4a3...|    2019-12-01|      09:00:01|\n",
      "+-------------------+----------+----------+-------------------+--------------------+-----+-------+---------+--------------------+--------------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_dict_seperated[\"2019_dec\"].show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+-------------------+--------------------+-------+-------+---------+--------------------+--------------+--------------+\n",
      "|         event_time|event_type|product_id|        category_id|       category_code|  brand|  price|  user_id|        user_session|event_time_ymd|event_time_hms|\n",
      "+-------------------+----------+----------+-------------------+--------------------+-------+-------+---------+--------------------+--------------+--------------+\n",
      "|2020-04-01 09:00:00|      view|   1201465|2232732101407408685|apparel.shoes.sli...|samsung| 230.38|568984877|e2456cef-2d4f-42b...|    2020-04-01|      09:00:00|\n",
      "|2020-04-01 09:00:01|      view|   1307156|2053013554658804075|electronics.audio...|  apple|1352.67|514955500|38f43134-de83-471...|    2020-04-01|      09:00:01|\n",
      "|2020-04-01 09:00:01|      view|   1480477|2053013563835941749|appliances.kitche...|  apple|1184.05|633645770|16aba270-b3c2-4b2...|    2020-04-01|      09:00:01|\n",
      "+-------------------+----------+----------+-------------------+--------------------+-------+-------+---------+--------------------+--------------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_dict_seperated[\"2020_apr\"].show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 데이터셋 parquet 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_csv_dict_to_parquet(csv_dict: dict, output_folder: str) -> None:\n",
    "    \"\"\"\n",
    "    딕셔너리의 각 Spark DataFrame을 parquet 파일로 저장\n",
    "    \"\"\"\n",
    "    # output_folder가 존재하지 않으면 생성\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    # 딕셔너리의 각 key-value 쌍에 대해 처리\n",
    "    for key, df in tqdm(csv_dict.items(), desc=\"Saving Parquet Files\"):\n",
    "\n",
    "        parquet_file = key + \".parquet\"\n",
    "        output_path = os.path.join(output_folder, parquet_file)\n",
    "        \n",
    "        # DataFrame을 parquet 파일로 저장 (overwrite 모드)\n",
    "        df.write.mode(\"overwrite\").parquet(output_path)\n",
    "        \n",
    "        print(f\"{key} 데이터프레임이 {output_path}로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Parquet Files:  14%|█▍        | 1/7 [01:40<10:05, 100.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019_dec 데이터프레임이 data\\parquet_data\\2019_dec.parquet로 저장되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Parquet Files:  29%|██▊       | 2/7 [03:25<08:34, 102.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019_nov 데이터프레임이 data\\parquet_data\\2019_nov.parquet로 저장되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Parquet Files:  43%|████▎     | 3/7 [04:28<05:40, 85.03s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019_oct 데이터프레임이 data\\parquet_data\\2019_oct.parquet로 저장되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Parquet Files:  57%|█████▋    | 4/7 [06:10<04:34, 91.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020_apr 데이터프레임이 data\\parquet_data\\2020_apr.parquet로 저장되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Parquet Files:  71%|███████▏  | 5/7 [07:34<02:57, 88.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020_feb 데이터프레임이 data\\parquet_data\\2020_feb.parquet로 저장되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Parquet Files:  86%|████████▌ | 6/7 [09:01<01:28, 88.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020_jan 데이터프레임이 data\\parquet_data\\2020_jan.parquet로 저장되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Parquet Files: 100%|██████████| 7/7 [10:26<00:00, 89.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020_mar 데이터프레임이 data\\parquet_data\\2020_mar.parquet로 저장되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output_folder = os.path.join(\"data\", \"parquet_data\")\n",
    "save_csv_dict_to_parquet(csv_dict_seperated, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkSession 종료\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 parquet 파일 확인: SparkSession 생성\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ParquetReader\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+--------------+--------------+\n",
      "|         event_time|event_type|product_id|        category_id|       category_code|   brand|  price|  user_id|        user_session|event_time_ymd|event_time_hms|\n",
      "+-------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+--------------+--------------+\n",
      "|2019-10-13 15:25:46|      view|   1002544|2053013555631882655|electronics.smart...|   apple| 460.51|518958788|e7e27c5c-1e78-481...|    2019-10-13|      15:25:46|\n",
      "|2019-10-13 15:25:46|      view|   3700301|2053013565983425517|appliances.enviro...|   vitek| 120.93|557977070|7afc206c-7259-4be...|    2019-10-13|      15:25:46|\n",
      "|2019-10-13 15:25:46|      view|  49100004|2127425375913902544|                NULL|    NULL|  45.05|514456508|9d6837a5-40df-49d...|    2019-10-13|      15:25:46|\n",
      "|2019-10-13 15:25:46|      view|   9200409|2053013552913973497|computers.periphe...|defender|  12.56|512530774|df2d048d-c1ae-41b...|    2019-10-13|      15:25:46|\n",
      "|2019-10-13 15:25:46|      view|   1306558|2053013558920217191|  computers.notebook|    acer|1801.82|523366823|0c7f0449-74d5-4b0...|    2019-10-13|      15:25:46|\n",
      "+-------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+--------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquet_2019_oct = spark.read.parquet(os.path.join(output_folder, \"2019_oct.parquet\"))\n",
    "parquet_2019_oct.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 2019_oct DataFrame shape: (42448764, 9)\n",
      "Parquet으로 저장된 2019_oct DataFrame shape: (42448764, 11)\n"
     ]
    }
   ],
   "source": [
    "print(\"원본 2019_oct DataFrame shape:\", (num_rows_oct, num_columns_oct))\n",
    "\n",
    "\n",
    "# 행의 개수\n",
    "num_rows_parquet_2019_oct = parquet_2019_oct.count()\n",
    "\n",
    "# 열의 개수: parquet_2019_oct.columns 리스트의 길이 계산\n",
    "num_columns_parquet_2019_oct = len(parquet_2019_oct.columns)\n",
    "\n",
    "print(\"Parquet으로 저장된 2019_oct DataFrame shape:\", (num_rows_parquet_2019_oct, num_columns_parquet_2019_oct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkSession 종료\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
