{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark를 활용한 ecommerce 유저 행동 데이터 parquet 파일 변환 및 저장\n",
    "- [kaggle 데이터셋 링크](https://www.kaggle.com/datasets/mkechinov/ecommerce-behavior-data-from-multi-category-store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.functions import to_timestamp, date_format, col, unix_timestamp\n",
    "from pyspark.sql.dataframe import DataFrame as SparkDataFrame\n",
    "from pyspark.sql import functions as F\n",
    "# from pyspark.sql.functions import lit\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2019-Dec.csv',\n",
       " '2019-Nov.csv',\n",
       " '2019-Oct.csv',\n",
       " '2020-Apr.csv',\n",
       " '2020-Feb.csv',\n",
       " '2020-Jan.csv',\n",
       " '2020-Mar.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_files = os.listdir(\"data/raw_data\")\n",
    "raw_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. 데이터프레임을 dictionary로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_files_to_dict(spark, folder_path):\n",
    "    \"\"\"\n",
    "    지정된 폴더 내의 모든 CSV 파일을 읽어와서,\n",
    "    파일명(key)과 DataFrame(value)의 dictionary를 생성\n",
    "    \"\"\"\n",
    "    # CSV 파일 목록 생성 (확장자가 .csv인 파일 필터링)\n",
    "    # csv_files = [f for f in os.listdir(folder_path) if f.endswith(\".csv\")]\n",
    "    csv_files = os.listdir(folder_path)\n",
    "    \n",
    "    # 결과를 저장할 dictionary 초기화\n",
    "    csv_dict = {}\n",
    "    \n",
    "    # 각 CSV 파일을 읽어와 dictionary에 저장\n",
    "    for file_name in tqdm(csv_files, desc=\"Processing CSV files\"):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        df = spark.read \\\n",
    "            .option(\"header\", True) \\\n",
    "            .option(\"inferSchema\", True) \\\n",
    "            .option(\"timestampFormat\", \"yyyy-MM-dd HH:mm:ss\")\\\n",
    "            .csv(file_path)\n",
    "        \n",
    "        raw_df_name = file_name.replace(\"-\", \"_\").replace(\".csv\", \"\").lower()\n",
    "        csv_dict[raw_df_name] = df\n",
    "        # DataFrame 내용 확인 (첫 5행 출력)\n",
    "        # df.show(5)\n",
    "    \n",
    "    return csv_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current TimeZone: Asia/Seoul\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files: 100%|██████████| 7/7 [02:32<00:00, 21.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CSV 파일 Dictionary keys: ['2019_dec', '2019_nov', '2019_oct', '2020_apr', '2020_feb', '2020_jan', '2020_mar']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# SparkSession 생성\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MultiCSVReader\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Current TimeZone:\", spark.conf.get(\"spark.sql.session.timeZone\"))\n",
    "\n",
    "# SparkSession Time Zone을 UTC로 설정\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "\n",
    "# 함수 호출하여 dictionary 생성\n",
    "csv_dict = load_csv_files_to_dict(spark, \"data/raw_data\")\n",
    "\n",
    "# 생성된 dictionary의 key (파일명) 출력\n",
    "print(\"\\nCSV 파일 Dictionary keys:\", list(csv_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current TimeZone: UTC\n"
     ]
    }
   ],
   "source": [
    "# Asia/Seoul에서 UTC로 바뀐 Time Zone 확인\n",
    "print(\"Current TimeZone:\", spark.conf.get(\"spark.sql.session.timeZone\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. 불러온 데이터셋 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- event_time: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- category_id: long (nullable = true)\n",
      " |-- category_code: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- user_session: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_dict[\"2019_oct\"].printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+-------------------+--------------------+--------+-----+---------+--------------------+\n",
      "|          event_time|event_type|product_id|        category_id|       category_code|   brand|price|  user_id|        user_session|\n",
      "+--------------------+----------+----------+-------------------+--------------------+--------+-----+---------+--------------------+\n",
      "|2019-10-01 00:00:...|      view|  44600062|2103807459595387724|                NULL|shiseido|35.79|541312140|72d76fde-8bb3-4e0...|\n",
      "|2019-10-01 00:00:...|      view|   3900821|2053013552326770905|appliances.enviro...|    aqua| 33.2|554748717|9333dfbd-b87a-470...|\n",
      "|2019-10-01 00:00:...|      view|  17200506|2053013559792632471|furniture.living_...|    NULL|543.1|519107250|566511c2-e2e3-422...|\n",
      "+--------------------+----------+----------+-------------------+--------------------+--------+-----+---------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_dict[\"2019_oct\"].show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+-------------------+--------------------+------+------+---------+--------------------+\n",
      "|          event_time|event_type|product_id|        category_id|       category_code| brand| price|  user_id|        user_session|\n",
      "+--------------------+----------+----------+-------------------+--------------------+------+------+---------+--------------------+\n",
      "|2019-11-01 00:00:...|      view|   1003461|2053013555631882655|electronics.smart...|xiaomi|489.07|520088904|4d3b30da-a5e4-49d...|\n",
      "|2019-11-01 00:00:...|      view|   5000088|2053013566100866035|appliances.sewing...|janome|293.65|530496790|8e5f4f83-366c-4f7...|\n",
      "|2019-11-01 00:00:...|      view|  17302664|2053013553853497655|                NULL| creed| 28.31|561587266|755422e7-9040-477...|\n",
      "+--------------------+----------+----------+-------------------+--------------------+------+------+---------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_dict[\"2019_nov\"].show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+-------------------+--------------------+-----+-------+---------+--------------------+\n",
      "|          event_time|event_type|product_id|        category_id|       category_code|brand|  price|  user_id|        user_session|\n",
      "+--------------------+----------+----------+-------------------+--------------------+-----+-------+---------+--------------------+\n",
      "|2019-12-01 00:00:...|      view|   1005105|2232732093077520756|construction.tool...|apple|1302.48|556695836|ca5eefc5-11f9-450...|\n",
      "|2019-12-01 00:00:...|      view|  22700068|2232732091643068746|                NULL|force| 102.96|577702456|de33debe-c7bf-44e...|\n",
      "|2019-12-01 00:00:...|      view|   2402273|2232732100769874463|appliances.person...|bosch| 313.52|539453785|5ee185a7-0689-4a3...|\n",
      "+--------------------+----------+----------+-------------------+--------------------+-----+-------+---------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_dict[\"2019_dec\"].show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+-----------+-------------+-------+-----+-------+------------+\n",
      "|event_time|event_type|product_id|category_id|category_code|  brand|price|user_id|user_session|\n",
      "+----------+----------+----------+-----------+-------------+-------+-----+-------+------------+\n",
      "|         0|         0|         0|          0|     13515609|6113008|    0|      0|           2|\n",
      "+----------+----------+----------+-----------+-------------+-------+-----+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_counts_oct = csv_dict[\"2019_oct\"].select(\n",
    "    [F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in csv_dict[\"2019_oct\"].columns]\n",
    ")\n",
    "null_counts_oct.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape: (42448764, 11)\n"
     ]
    }
   ],
   "source": [
    "# 행의 개수\n",
    "num_rows_oct = csv_dict[\"2019_oct\"].count()\n",
    "\n",
    "# 열의 개수: csv_dict[\"2019_oct\"].columns 리스트의 길이 계산\n",
    "num_columns_oct = len(csv_dict[\"2019_oct\"].columns)\n",
    "\n",
    "print(\"DataFrame shape:\", (num_rows_oct, num_columns_oct))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+-----------+-------------+-------+-----+-------+------------+\n",
      "|event_time|event_type|product_id|category_id|category_code|  brand|price|user_id|user_session|\n",
      "+----------+----------+----------+-----------+-------------+-------+-----+-------+------------+\n",
      "|         0|         0|         0|          0|      6755873|8985057|    0|      0|         109|\n",
      "+----------+----------+----------+-----------+-------------+-------+-----+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_counts_apr = csv_dict[\"2020_apr\"].select(\n",
    "    [F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in csv_dict[\"2020_apr\"].columns]\n",
    ")\n",
    "null_counts_apr.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape: (66589268, 9)\n"
     ]
    }
   ],
   "source": [
    "# 행의 개수\n",
    "num_rows_apr = csv_dict[\"2020_apr\"].count()\n",
    "\n",
    "# 열의 개수: csv_dict[\"2020_apr\"].columns 리스트의 길이 계산\n",
    "num_columns_apr = len(csv_dict[\"2020_apr\"].columns)\n",
    "\n",
    "print(\"DataFrame shape:\", (num_rows_apr, num_columns_apr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 파생 컬럼 생성\n",
    "- `event_time` 컬럼의 형태: `2019-12-01 09:00:00`\n",
    "- 이 컬럼을 날짜와 시간 컬럼으로 각각 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_event_time(csv_dict: dict, event_time_col: str = \"event_time\") -> dict:\n",
    "    \"\"\"\n",
    "    Spark DataFrame의 event_time 컬럼을 timestamp로 변환 후, \n",
    "    날짜(yyyy-MM-dd)와 시간(HH:mm:ss) 컬럼으로 분리하여 csv_dict의 각 DataFrame을 업데이트\n",
    "    \"\"\"\n",
    "    for key in tqdm(csv_dict.keys(), desc=\"Seperating Columns\"):\n",
    "        df: SparkDataFrame = csv_dict[key]\n",
    "        \n",
    "        # 1. event_time 컬럼을 to_timestamp()를 사용하여 timestamp 타입으로 변환\n",
    "        df = df.withColumn(event_time_col, F.to_timestamp(F.col(event_time_col)))\n",
    "        \n",
    "        # 2. 날짜 부분 추출: date_format()을 사용하여 \"yyyy-MM-dd\" 형식으로 날짜 추출\n",
    "        df = df.withColumn(\"event_time_ymd\", F.date_format(F.col(event_time_col), \"yyyy-MM-dd\"))\n",
    "        \n",
    "        # 3. 시간 부분 추출: date_format()을 사용하여 \"HH:mm:ss\" 형식으로 시간 추출\n",
    "        df = df.withColumn(\"event_time_hms\", F.date_format(F.col(event_time_col), \"HH:mm:ss\"))\n",
    "        \n",
    "        # 수정된 DataFrame을 dictionary에 업데이트\n",
    "        csv_dict[key] = df\n",
    "        \n",
    "    return csv_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seperating Columns: 100%|██████████| 7/7 [00:00<00:00, 38.11it/s]\n"
     ]
    }
   ],
   "source": [
    "csv_dict_seperated = separate_event_time(csv_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['2019_dec', '2019_nov', '2019_oct', '2020_apr', '2020_feb', '2020_jan', '2020_mar'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_dict_seperated.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- category_id: long (nullable = true)\n",
      " |-- category_code: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- user_session: string (nullable = true)\n",
      " |-- event_time_ymd: string (nullable = true)\n",
      " |-- event_time_hms: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_dict_seperated[\"2019_dec\"].printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+-------------------+--------------------+-----+-------+---------+--------------------+--------------+--------------+\n",
      "|         event_time|event_type|product_id|        category_id|       category_code|brand|  price|  user_id|        user_session|event_time_ymd|event_time_hms|\n",
      "+-------------------+----------+----------+-------------------+--------------------+-----+-------+---------+--------------------+--------------+--------------+\n",
      "|2019-12-01 00:00:00|      view|   1005105|2232732093077520756|construction.tool...|apple|1302.48|556695836|ca5eefc5-11f9-450...|    2019-12-01|      00:00:00|\n",
      "|2019-12-01 00:00:00|      view|  22700068|2232732091643068746|                NULL|force| 102.96|577702456|de33debe-c7bf-44e...|    2019-12-01|      00:00:00|\n",
      "|2019-12-01 00:00:01|      view|   2402273|2232732100769874463|appliances.person...|bosch| 313.52|539453785|5ee185a7-0689-4a3...|    2019-12-01|      00:00:01|\n",
      "+-------------------+----------+----------+-------------------+--------------------+-----+-------+---------+--------------------+--------------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_dict_seperated[\"2019_dec\"].show(3)\n",
    "# csv_dict_seperated[\"2019_dec\"].select(\"event_time\").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+-------------------+--------------------+-----+-------+---------+--------------------+--------------+--------------+\n",
      "|         event_time|event_type|product_id|        category_id|       category_code|brand|  price|  user_id|        user_session|event_time_ymd|event_time_hms|\n",
      "+-------------------+----------+----------+-------------------+--------------------+-----+-------+---------+--------------------+--------------+--------------+\n",
      "|2019-12-01 00:00:00|      view|   1005105|2232732093077520756|construction.tool...|apple|1302.48|556695836|ca5eefc5-11f9-450...|    2019-12-01|      00:00:00|\n",
      "|2019-12-01 00:00:00|      view|  22700068|2232732091643068746|                NULL|force| 102.96|577702456|de33debe-c7bf-44e...|    2019-12-01|      00:00:00|\n",
      "|2019-12-01 00:00:01|      view|   2402273|2232732100769874463|appliances.person...|bosch| 313.52|539453785|5ee185a7-0689-4a3...|    2019-12-01|      00:00:01|\n",
      "+-------------------+----------+----------+-------------------+--------------------+-----+-------+---------+--------------------+--------------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_dict_seperated[\"2019_dec\"].orderBy(\"event_time\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+-------------------+--------------------+-------+-------+---------+--------------------+--------------+--------------+\n",
      "|         event_time|event_type|product_id|        category_id|       category_code|  brand|  price|  user_id|        user_session|event_time_ymd|event_time_hms|\n",
      "+-------------------+----------+----------+-------------------+--------------------+-------+-------+---------+--------------------+--------------+--------------+\n",
      "|2020-01-01 00:00:00|      view|   1005073|2232732093077520756|construction.tool...|samsung|1130.02|519698804|69b5d72f-fd6e-4fe...|    2020-01-01|      00:00:00|\n",
      "|2020-01-01 00:00:01|      view|   1005192|2232732093077520756|construction.tool...|  meizu| 205.67|527767423|7f596032-ccbf-464...|    2020-01-01|      00:00:01|\n",
      "|2020-01-01 00:00:01|      view| 100063693|2053013552427434207|       apparel.shirt| turtle| 136.43|519046195|d1e2f343-84bb-49b...|    2020-01-01|      00:00:01|\n",
      "+-------------------+----------+----------+-------------------+--------------------+-------+-------+---------+--------------------+--------------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_dict_seperated[\"2020_jan\"].show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+-------------------+--------------------+-------+-------+---------+--------------------+--------------+--------------+\n",
      "|         event_time|event_type|product_id|        category_id|       category_code|  brand|  price|  user_id|        user_session|event_time_ymd|event_time_hms|\n",
      "+-------------------+----------+----------+-------------------+--------------------+-------+-------+---------+--------------------+--------------+--------------+\n",
      "|2020-04-01 00:00:00|      view|   1201465|2232732101407408685|apparel.shoes.sli...|samsung| 230.38|568984877|e2456cef-2d4f-42b...|    2020-04-01|      00:00:00|\n",
      "|2020-04-01 00:00:01|      view|   1307156|2053013554658804075|electronics.audio...|  apple|1352.67|514955500|38f43134-de83-471...|    2020-04-01|      00:00:01|\n",
      "|2020-04-01 00:00:01|      view|   1480477|2053013563835941749|appliances.kitche...|  apple|1184.05|633645770|16aba270-b3c2-4b2...|    2020-04-01|      00:00:01|\n",
      "+-------------------+----------+----------+-------------------+--------------------+-------+-------+---------+--------------------+--------------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_dict_seperated[\"2020_apr\"].show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 데이터셋 parquet 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1. 변환된 parquet 파일 저장 및 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_csv_dict_to_parquet(csv_dict: dict, output_folder: str) -> None:\n",
    "    \"\"\"\n",
    "    딕셔너리의 각 Spark DataFrame을 parquet 파일로 저장\n",
    "    \"\"\"\n",
    "    # output_folder가 존재하지 않으면 생성\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    # 딕셔너리의 각 key-value 쌍에 대해 처리\n",
    "    for key, df in tqdm(csv_dict.items(), desc=\"Saving Parquet Files\"):\n",
    "\n",
    "        parquet_file = key + \"_parquet\"\n",
    "        output_path = os.path.join(output_folder, parquet_file)\n",
    "        \n",
    "        # DataFrame을 parquet 파일로 저장 (overwrite 모드)\n",
    "        df.write.mode(\"overwrite\").parquet(output_path)\n",
    "        \n",
    "        print(f\"{key} 데이터프레임이 {output_path}로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Parquet Files:  14%|█▍        | 1/7 [01:47<10:42, 107.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019_dec 데이터프레임이 data\\parquet_data\\2019_dec_parquet로 저장되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Parquet Files:  29%|██▊       | 2/7 [03:32<08:49, 105.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019_nov 데이터프레임이 data\\parquet_data\\2019_nov_parquet로 저장되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Parquet Files:  43%|████▎     | 3/7 [04:37<05:48, 87.16s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019_oct 데이터프레임이 data\\parquet_data\\2019_oct_parquet로 저장되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Parquet Files:  57%|█████▋    | 4/7 [06:19<04:39, 93.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020_apr 데이터프레임이 data\\parquet_data\\2020_apr_parquet로 저장되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Parquet Files:  71%|███████▏  | 5/7 [07:47<03:02, 91.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020_feb 데이터프레임이 data\\parquet_data\\2020_feb_parquet로 저장되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Parquet Files:  86%|████████▌ | 6/7 [09:16<01:30, 90.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020_jan 데이터프레임이 data\\parquet_data\\2020_jan_parquet로 저장되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Parquet Files: 100%|██████████| 7/7 [10:45<00:00, 92.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020_mar 데이터프레임이 data\\parquet_data\\2020_mar_parquet로 저장되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output_folder = os.path.join(\"data\", \"parquet_data\")\n",
    "save_csv_dict_to_parquet(csv_dict_seperated, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+--------------+--------------+\n",
      "|         event_time|event_type|product_id|        category_id|       category_code|   brand|  price|  user_id|        user_session|event_time_ymd|event_time_hms|\n",
      "+-------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+--------------+--------------+\n",
      "|2019-10-13 06:25:46|      view|   1002544|2053013555631882655|electronics.smart...|   apple| 460.51|518958788|e7e27c5c-1e78-481...|    2019-10-13|      06:25:46|\n",
      "|2019-10-13 06:25:46|      view|   3700301|2053013565983425517|appliances.enviro...|   vitek| 120.93|557977070|7afc206c-7259-4be...|    2019-10-13|      06:25:46|\n",
      "|2019-10-13 06:25:46|      view|  49100004|2127425375913902544|                NULL|    NULL|  45.05|514456508|9d6837a5-40df-49d...|    2019-10-13|      06:25:46|\n",
      "|2019-10-13 06:25:46|      view|   9200409|2053013552913973497|computers.periphe...|defender|  12.56|512530774|df2d048d-c1ae-41b...|    2019-10-13|      06:25:46|\n",
      "|2019-10-13 06:25:46|      view|   1306558|2053013558920217191|  computers.notebook|    acer|1801.82|523366823|0c7f0449-74d5-4b0...|    2019-10-13|      06:25:46|\n",
      "+-------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+--------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquet_2019_oct = spark.read.parquet(os.path.join(output_folder, \"2019_oct_parquet\"))\n",
    "parquet_2019_oct.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 2019_oct DataFrame shape: (42448764, 11)\n",
      "Parquet으로 저장된 2019_oct DataFrame shape: (42448764, 11)\n"
     ]
    }
   ],
   "source": [
    "print(\"원본 2019_oct DataFrame shape:\", (num_rows_oct, num_columns_oct))\n",
    "\n",
    "\n",
    "# 행의 개수\n",
    "num_rows_parquet_2019_oct = parquet_2019_oct.count()\n",
    "\n",
    "# 열의 개수: parquet_2019_oct.columns 리스트의 길이 계산\n",
    "num_columns_parquet_2019_oct = len(parquet_2019_oct.columns)\n",
    "\n",
    "print(\"Parquet으로 저장된 2019_oct DataFrame shape:\", (num_rows_parquet_2019_oct, num_columns_parquet_2019_oct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. parquet 파일 하나로 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_merged_csv_dict_to_parquet(csv_dict: dict, output_folder: str, spark: SparkSession) -> None:\n",
    "    \"\"\"\n",
    "    여러 Spark DataFrame을 하나의 DataFrame으로 합쳐 Parquet 파일로 저장하고, Merge가 제대로 되었는지 확인\n",
    "    \"\"\"\n",
    "    # 모든 데이터프레임을 리스트로 수집\n",
    "    df_list = []\n",
    "    total_rows = 0  # 병합 전 총 행 수\n",
    "    column_counts = {}  # 개별 데이터프레임의 컬럼 개수 저장\n",
    "\n",
    "    print(\"\\n===== 개별 DataFrame 정보 =====\")\n",
    "    \n",
    "    for key, df in tqdm(csv_dict.items(), desc=\"Appending DataFrames\"):\n",
    "        row_count = df.count()  # 개별 DataFrame의 행 수\n",
    "        col_count = len(df.columns)  # 개별 DataFrame의 열 수\n",
    "        total_rows += row_count  # 전체 행 수 누적\n",
    "\n",
    "        # 개별 DataFrame shape 저장\n",
    "        column_counts[key] = (row_count, col_count)\n",
    "\n",
    "        # key별 shape 출력\n",
    "        print(f\"{key}: {row_count} rows, {col_count} columns\")\n",
    "\n",
    "        # 원본 파일명을 'source_file' 컬럼으로 추가 (데이터 출처 확인 가능)\n",
    "        df = df.withColumn(\"source_file\", F.lit(key))\n",
    "        df_list.append(df)\n",
    "\n",
    "    # 데이터프레임 리스트를 하나로 병합\n",
    "    merged_df = df_list[0]\n",
    "    for df in tqdm(df_list[1:], desc=\"Merging DataFrames\"):\n",
    "        merged_df = merged_df.unionByName(df, allowMissingColumns=True)\n",
    "\n",
    "    # 병합 후 최종 DataFrame의 shape\n",
    "    merged_rows = merged_df.count()\n",
    "    merged_columns = len(merged_df.columns)\n",
    "\n",
    "    # 저장할 Parquet 파일 경로\n",
    "    output_path = os.path.join(output_folder, \"total_merged_parquet\")\n",
    "\n",
    "    # Parquet 파일로 저장 (overwrite 모드)\n",
    "    merged_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "    # Merge가 제대로 되었는지 확인\n",
    "    print(\"\\n============== Merge 결과 ==============\")\n",
    "    print(f\"개별 DataFrame 총 행 수 합계: {total_rows}, 병합된 DataFrame 행 수: {merged_rows}\")\n",
    "    print(f\"개별 DataFrame 최대 컬럼 수: {max(column_counts.values(), key=lambda x: x[1])[1]}, 병합된 DataFrame 컬럼 수: {merged_columns}\\n\")\n",
    "    \n",
    "    if total_rows == merged_rows:\n",
    "        print(\"병합이 정상적으로 이루어졌습니다!\")\n",
    "    else:\n",
    "        print(\"병합된 행 수가 개별 행 수 총합과 일치하지 않습니다. 데이터 손실 가능성 존재!\")\n",
    "\n",
    "    print(f\"\\n모든 데이터프레임이 병합되어 {output_path}에 저장되었습니다.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== 개별 DataFrame 정보 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Appending DataFrames:  14%|█▍        | 1/7 [00:06<00:41,  6.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019_dec: 67542878 rows, 11 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Appending DataFrames:  29%|██▊       | 2/7 [00:13<00:33,  6.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019_nov: 67501979 rows, 11 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Appending DataFrames:  43%|████▎     | 3/7 [00:17<00:21,  5.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019_oct: 42448764 rows, 11 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Appending DataFrames:  57%|█████▋    | 4/7 [00:23<00:17,  5.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020_apr: 66589268 rows, 11 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Appending DataFrames:  71%|███████▏  | 5/7 [00:28<00:10,  5.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020_feb: 55318565 rows, 11 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Appending DataFrames:  86%|████████▌ | 6/7 [00:33<00:05,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020_jan: 55967041 rows, 11 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Appending DataFrames: 100%|██████████| 7/7 [00:39<00:00,  5.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020_mar: 56341241 rows, 11 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging DataFrames: 100%|██████████| 6/6 [00:00<00:00, 120.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============== Merge 결과 ==============\n",
      "개별 DataFrame 총 행 수 합계: 411709736, 병합된 DataFrame 행 수: 411709736\n",
      "개별 DataFrame 최대 컬럼 수: 11, 병합된 DataFrame 컬럼 수: 12\n",
      "\n",
      "병합이 정상적으로 이루어졌습니다!\n",
      "\n",
      "모든 데이터프레임이 병합되어 data\\parquet_data\\total_merged_parquet에 저장되었습니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_folder = os.path.join(\"data\", \"parquet_data\")\n",
    "save_merged_csv_dict_to_parquet(csv_dict_seperated, output_folder, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+--------------+--------------+-----------+\n",
      "|         event_time|event_type|product_id|        category_id|       category_code|   brand|  price|  user_id|        user_session|event_time_ymd|event_time_hms|source_file|\n",
      "+-------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+--------------+--------------+-----------+\n",
      "|2019-11-17 08:43:00|      view|   2501799|2053013564003713919|appliances.kitche...|elenberg|  46.31|563237118|4368d099-6d19-47c...|    2019-11-17|      08:43:00|   2019_nov|\n",
      "|2019-11-17 08:43:00|      view|   6400335|2053013554121933129|computers.compone...|   intel| 435.28|551129779|4db2c365-ee85-443...|    2019-11-17|      08:43:00|   2019_nov|\n",
      "|2019-11-17 08:43:00|      view|   3701538|2053013565983425517|appliances.enviro...|  irobot|1878.81|539845715|bf7d95c0-69e1-40f...|    2019-11-17|      08:43:00|   2019_nov|\n",
      "|2019-11-17 08:43:00|      view|  26400266|2053013563651392361|                NULL| lucente| 119.18|572211322|8e6c63f8-7f34-48b...|    2019-11-17|      08:43:00|   2019_nov|\n",
      "|2019-11-17 08:43:00|      view|   1004659|2053013555631882655|electronics.smart...| samsung| 762.18|512965259|2981c9f9-3905-49d...|    2019-11-17|      08:43:00|   2019_nov|\n",
      "+-------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+--------------+--------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquet_df = spark.read.parquet(os.path.join(output_folder, \"total_merged_parquet\"))\n",
    "parquet_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+--------------+--------------+-----------+\n",
      "|         event_time|event_type|product_id|        category_id|       category_code|   brand|  price|  user_id|        user_session|event_time_ymd|event_time_hms|source_file|\n",
      "+-------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+--------------+--------------+-----------+\n",
      "|2019-10-01 00:00:00|      view|  44600062|2103807459595387724|                NULL|shiseido|  35.79|541312140|72d76fde-8bb3-4e0...|    2019-10-01|      00:00:00|   2019_oct|\n",
      "|2019-10-01 00:00:00|      view|   3900821|2053013552326770905|appliances.enviro...|    aqua|   33.2|554748717|9333dfbd-b87a-470...|    2019-10-01|      00:00:00|   2019_oct|\n",
      "|2019-10-01 00:00:01|      view|  17200506|2053013559792632471|furniture.living_...|    NULL|  543.1|519107250|566511c2-e2e3-422...|    2019-10-01|      00:00:01|   2019_oct|\n",
      "|2019-10-01 00:00:01|      view|   1307067|2053013558920217191|  computers.notebook|  lenovo| 251.74|550050854|7c90fc70-0e80-459...|    2019-10-01|      00:00:01|   2019_oct|\n",
      "|2019-10-01 00:00:04|      view|   1004237|2053013555631882655|electronics.smart...|   apple|1081.98|535871217|c6bd7419-2748-4c5...|    2019-10-01|      00:00:04|   2019_oct|\n",
      "+-------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+--------------+--------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquet_df.orderBy(\"event_time\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkSession 종료\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
